---
title : "[Big Data]Spark4"
data : 2021-02-05 00:15:28 -0400
categories : 프로그래머스인공지능스쿨
use_math: true
---
# Spark MLlib 모델 튜닝: Spark MLlib가 제공해주는 모델 튜닝에 대해 배워보자
## Spark MLlib 모델 튜닝 (ML Tuning)
- 최적의 하이퍼 파라미터를 선택하는 것
    - 최적의 모델 혹은 모델의 파라미터를 찾는 것이 아주 중요
    - epoch, 학습률, decision tree의 최대 깊이 등이 하이퍼 파라미터가 될 수 있다.
    - 하나씩 테스트해보는 것 vs. 다수를 동시에 테스트하는 것
    - 모델 선택의 중요한 부분은 테스트 방법
        - 교차 검증(Cross Validation)과 홀드 아웃(Train-Validation Split) 테스트 방법을 지원
    - 보통 ML Pipeline과 같이 사용함.
<br>

## Spark MLlib 모델 테스트
- 모델 테스트 방법: 크게 2가지가 존재
    - 교차 검증(Cross Validation)
    - 훈련/테스트셋 나누기(Train-validation split)
- 훈련용과 테스트용 데이터 기반 테스트(Train & Validation Split)
    - Holdout 테스트라고 하기도 함
    - 보통 훈련과 테스트 비율을 80:20 or 75:25로 한다.
    1. 트레이닝셋을 훈련용과 테스트용으로 분리
    2. 훈련용 데이터로 모델 빌딩
    3. 테스트용 데이터로 예측값 수집
    4. 예측값과 정답값 비교
    - 문제는 훈련용과 테스트용의 밸런스가 비슷하지 않으면 만들어진 모델과 이를 테스트할때 문제가 생길 수 있다(오버피팅)
    - 이를 해결하기 위한 것이 바로 교차분석 테스트(CV)
- 교차분석 테스트(Cross Validation)
    - K-Fold 테스트라고 부르기도 함
    - 트레이닝 셋을 K개의 서브셋으로 나눠 총 K번의 훈련을 반복. 각 K를 폴드라고 부른다. 모든 폴드가 한번씩 훈련과 테스트 데이터가 되고, 최종적으로 K개의 결과에 대한 평균이 Output이 된다.
    - 홀드아웃 테스트보다 훨씬 더 안정적. 오버피팅 문제가 감소
<br>

## Spark MLlib 모델 튜닝(Tuning)
- **Cross Validation, TrainValidationSplit**
    - TrainValidationSplit: 홀드아웃 기반 테스트 수행
    - CrossValidatior: 교차분석(K-Fold) 기반 테스트 수행
    - 다음과 같은 입력을 기반으로 가장 좋은 파라미터를 찾아줌
        - Estimator: 머신러닝 모델 혹은 ML Pipeline
        - Evaluator: 머신러닝 모델의 성능을 나타내는 지표
        - Parameter: 훈련 반복 회수 등의 하이퍼 파라미터
            - ParamGridBuilder를 이용해 ParamGrid 타입의 변수 생성
    - 최종적으로 가장 결과가 좋은 모델을 리턴!


