---
title : "[인공지능 수학 - 선형대수]9강 : 벡터공간과 최소제곱법"
data : 2020-12-09 00:15:28 -0400
categories : 프로그래머스인공지능스쿨
use_math: true
---
# 벡터공간과 최소제곱법
## 집합
집합(set)은 임의의 원소(element)를 수집하여 만든 모듬이다. 어떤 방식으로 모아도 되니, 다음과 같은 집합도 생각할 수 있다.  
{수학, 3.14, 강아지, (1,2)}  
  
## 연산에 닫혀 있는 집합(set)
**집합이 연산에 닫혀 있다**는 개념이 있다. 어떤 연산을 생각한 다음, 집합에서 임의의 원소를 뽑아 연산을 수행한 결과가 여전히 집합의 원소로 있다면, 해당 집합은 연산에 닫혀 있다고 한다.  
다음 집합은 덧셈 연산에 닫혀 있다. 곱셈 연산에도 닫혀 있다.  
${x | x \in R}$  
다음 집합은 덧셈 연산에 닫혀 있지 않다. 하지만 곱셈 연산에는 닫혀 있다.  
{-1, 0, 1}  
  
## 공간(space)
**공간(space)은 다음의 두 연산에 닫혀 있는 집합이다.**  
- **덧셈연산에 닫혀 있다** : 집합에서 임의의 두 원소 x, y를 뽑아 더해도 그 결과 x + y는 집합의 원소이다.
- **스칼라 곱 연산에 닫혀 있다** : 집합에서 임의의 한 원소 x를 뽑아 임의의 스칼라 s배 한 결과 sx는 집합의 원소이다.
다음과 같이 n-벡터의 집합은 모두 공간이다.  
$R^n = {x|x = (x_1, x_2, \dots, x_n), (단, x_i \in R)}$  
앞으로 **모든 n-벡터 집합 $R^n$은 n차원 벡터 공간(vector space)라 부를 수 있다.  
  
## Column Space(열공간)
**행렬 A의 열벡터들에 대한 가능한 모든 선형조합의 결과를 모아 집합으로 구성**할 수 있을 것이다. 이 집합을 **column space(열공간)**이라 하고 다음과 같이 표기한다.  
![Alt Text](/assets/images/20201209/18.png)  
  
**Consistent Linear System**  
선형시스템 $Ax = b$가 해를 가지면(consistent), 다음을 만족한다.  
$b \in col(A)$  
해가 있는 선형시스템  
![Alt Text](/assets/images/20201209/19.png)  
  
**Inconsistent Linear System**  
선형시스템 $Ax = b$가 해가 없으면(inconsistent), 다음을 만족한다.  
$b \notin col(A)$  
해가 없는 선형시스템  
![Alt Text](/assets/images/20201209/20.png)  
  


# 최소제곱법
## 열공간(column space)으로 투영
선형시스템 $Ax = b$에 대한 **해가 없음에도 불구하고, 우리가 할 수 있는 최선이 무엇인가**를 생각해 보자.  
행렬 A가 정의하는 열공간에서 우리의 목표 b와 가장 가까운 지점은 b를 열공간에 투영(projection)한 지점일 것이다. 즉, **달성 가능한 최선의 목표 $proj_wb$**를 생각할 수 있다.  
최소제곱법(least square method)의 의미를 그림으로 나타내면 다음과 같다.  
![Alt Text](/assets/images/20201209/21.png)  
b는 도달하지 못하는 목표이지만, b와 가장 근사한 지점을 b를 w(column space)에 투영하여 찾는 것이다.  
  
## 최소제곱법(least squares method)
최소제곱법은 선형시스템 $Ax = b$에 대한 해 x가 없음에도 불구하고, 할 수 있는 최선의 대안을 내놓는 기법이다. 최소제곱법은 원래의 선형시스템 $Ax = b$가 아닌 다음의 선형시스템을 해결한다.  
$A\bar x = \bar b$ (단, $\bar b = proj_wb$)  
이 방법은 목표 $b$와 달성가능한 목표 $\bar b$의 차이를 나타내는 벡터 $(b - \bar b)$의 제곱길이를 최소화시키는 의미를 가지기 때문에 최소제곱법(least squares method)이라 불린다.  
  
주어진 선형시스템의 양변에 전치행렬 $A^T$를 곱하면 최소제곱법의 해를 구할 수 있다.  
$Ax = b$  
->$A^TA\bar x = A^Tb$  
->$\bar x = (A^TA)^-1A^Tb$  
  
최소제곱법으로 구한 해 $\bar x$는 원래의 선형시스템을 만족하는 해는 아니다.  
$A\bar x \neq b$  
  
최소제곱법으로 구한 해 $\bar x$는 다음을 만족하는 근사해(approximate solution)이다.  
$A\bar x = proj_wb$  
  
## 최소제곱법(least squares method)의 응용 : 선형회귀(Linear Regression)
2차원 공간에 m개의 정점이 그림과 같이 있을 때, 이를 잘 설명할 수 있는 직선 y = mx+b를 구하는 문제를 생각해 보자. 이를 선형회귀(linear regression) 문제라 한다.  
![Alt Text](/assets/images/20201209/22.png)  
  
선형회귀 문제는 다음과 같이 최소제곱법으로 풀 수 있다.  
1. 선형시스템 구성 : 직선이 각 정점을 모두 지나간다고 가정하고 선형시스템 $Ax = b$ 구성 (단, 주어진 모든 정점을 지나가는 직선은 존재하지 않으므로 선형시스템의 해는 존재하지 않음.)  
일단 위 그림에서의 모든 정점이 직선 y=mx+b를 지난다고 가정하고 값을 모두 대입해 행렬로 만들면 다음과 같다.  
$
\begin{bmatrix}
-3 & 1\cr
-1 & 1\cr
1 & 1\cr
3 & 1\cr
\end{bmatrix}
\begin{bmatrix}
m\cr
b\cr
\end{bmatrix}
=
\begin{bmatrix}
-1\cr
-1\cr
3\cr
3\cr
\end{bmatrix}
$  
2. 최소제곱법 적용 : $A^TA\bar x = A^tb$를 생각하고, $\begin{bmatrix}
\bar m\cr
\bar b\cr
\end{bmatrix}$를 구한다.  
이 예제에서는 transpose된 행렬이 $2 * 4$인데, 이를 좌우변에 곱해주면 좌변은 $2 * 2$, 우변은 $2 * 1$의 문제로 바뀐다. 문제가 아예 바뀌어버린다.