---
title : "[NLP]문서분류"
data : 2021-02-17 00:15:28 -0400
categories : 프로그래머스인공지능스쿨
use_math: true
---
# 자연어 처리: 문서분류
## 문서분류(Text Classification)
- 문서분류란?
    - 텍스트를 입력으로 받아 텍스트가 어떤 종류의 범주에 속하는지를 구분하는 작업
- 다양한 문서 분류 주제들
    - 문서의 범주, 주제 분류
        - 예: CS 논문의 CS 주제분류
    - 이메일 스팸 분류
    - 감성 분류
        - 예
            - 영화: 리뷰가 긍정적/부정적?
            - 제품: 새로운 아이폰에 대한 대중의 반응?
            - 정치: 이 정치인에 대한 사람들의 생각?
            - 예측: 감성 분류를 기반으로 선거 결과 예측
        - 감성의 여러 측면이 있을 수 있지만 여기선 간단한 작업에 집중
            - 감정적, 태도적, 성격적 측면
            - 이 텍스트는 긍정적인 태도/부정적인 태도인지
    - 언어 분류
<br>

## 문서분류: 정의
Input:  
- a document d (문서 d)
- a fixed set of classes $C = {c_1, c_2, ..., c_j}$ (가능한 모든 클래스들의 집합 C)  
Output: a predicted class $c \in C$(멀티레이블인 경우 아웃풋이 여러개의 클래스)  
<br>

## 문서 분류 방법들 - 규칙 기반 모델
- 단어들의 조합을 사용한 규칙들을 사용
    - 예: 스팸을 보내는 이메일들의 리스트를 만들어서 블랙리스트에 저장 or 이메일 내용 안에 "dollars"혹은 "당신은 선택되었습니다"등이 포함되면 블랙리스트 저장
- Precision은 높지만 recall이 낮음
    - 사람이 규칙을 만들기 때문에 정확도가 높음
    - 하지만 예외를 해결하기 위해 조건을 계속 추가하다 보면 결국 단일 조합으로 커버하지 못하는 경우가 생김. 
    - 사람의 손으로 규칙을 일일히 만드는 것은 비효율적이지만, 데이터가 없으면 초반에 규칙을 손수 만들어 baseline 모델을 만드는 것이 좋다.
- Snorkel 모델(규칙기반 + 머신러닝)
    - 각각의 규칙을 "labeling function"으로 간주
    - Graphical model의 일종인 factor graph를 사용해서 확률적 목표값을 생성하는 generative model
    - 프로젝트 초기 labeled data가 부족하거나 클래스 정의 자체가 애매한 경우(하지만 규칙을 생성하는 것은 쉽다고 가정)에 매우 유용한 방법
    - 확률적 목표값이 생성된 후엔 딥모델 등 다양한 모델을 사용가능
<br>

## 문서 분류 방법들 - 지도학습(클래스가 주어져 있는 경우)
Input:  
- a document d
- a fixed set of classes $C = {c_1, c_2, ..., c_j}$
- A training set of m hand-labeled documents $(d_1, c_1), ..., (d_m, c_m)$ (document 하나당 클래스 하나)
Output:  
- a learned classifier (분류기를 아웃풋으로 내놓는다)
<br>

- 다양한 모델 사용 가능
    - Naive Bayes: parametric model, 문제를 풀기 위해 학습해야할 파라미터들이 있다.
    - Logistic regression
    - Neural networks
    - k-Nearest Neighbors: non parametric model, 학습 단계가 따로 있는 것이 아니라 inference를 바로 할 수 있다. 하나의 새로운 문서가 주어졌을 때 주어진 레이블되어있는 문서들에 대해서 새로 주어진 문서와의 거리를 계산한다. 주어진 문서와 가장 가까운 레이블된 k개의 문서를 찾아 투표한다. 다수의 레이블이 새로운 데이터에 대한 최종 레이블이 된다.
<br>

## Naive Bayes 분류기
- Naive Bayes 가정과 Bag of Words 표현에 기반한 간단한 모델
<br>

## Bag of Words 표현
가방 안에 단어를 담은 것. 순서를 고려하지 않은 단어들의 집합. 대신 각각의 단어들이 나타나는 빈도수를 계산한다.  

## Naive Bayes 분류기 - 수식화
- 문서 d와 클래스 c
    - 문서 d가 주어졌을 때 특정 클래스 c의 확률
        - $P(c|d) = {P(d|c)P(c) \above 1pt P(d)}$
    - 이 값을 최대화 시키는 클래스 c를 찾아야 한다.
        - $C_{MAP} = argmax P(c|d) = argmax {P(d|c)P(c) \above 1pt P(d)} = argmax P(d|c)P(c)$
            - MAP은 가장 확률이 높은 클래스를 말한다.
            - 분모 $P(d)$ 부분은 클래스 c와 아무런 상관이 없으므로 뺀다.
            - $P(d|c)$는 likelihood이고 $P(c)$는 Prior이다.
        - $C_{MAP} = argmax P(x_1, x_2, ..., x_n|c)P(c)$
            - 데이터가 아주 크지 않다면 신뢰할 만한 정확도로 계산하기 힘들 것이다.
            - 클래스 c가 얼마나 자주 일어나는가?
            - 말뭉치 내에서의 상대적 빈도로 계산 가능
- Bag of Words 가정: 단어의 위치는 확률에 영향을 주지 않는다. 
- 조건부독립 가정: 클래스가 주어지면 속성들은 독립적이다. 조건부독립 관계를 그래피컬하게 표현하는 것을 베이지안 네트워크라고 하는데, 이것의 간단한 형태를 conditional independense라고 한다.
- Naive Bayes 분류기는 입력값에 관한 선형모델이다. 
    - $G_{NB} = argmax_{c_j \in C} P(c_j) \prod_{j\in positions} P(x_i|c_js)$
    - 모델 파라미터는?
- 확률적 생성 모델 복습
