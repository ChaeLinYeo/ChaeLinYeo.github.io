---
title : "[Deep Learning: 신경망의 기초]딥러닝 기초"
data : 2021-01-22 00:15:28 -0400
categories : 프로그래머스인공지능스쿨
use_math: true
---
# 심층학습 최적화
훈련집합으로 학습을 마친 후, 현장에서 발생하는 새로운 샘플을 잘 예측해야함. ->즉, **일반화**능력이 좋아야함
- 훈련집합은 전체 데이터(실제, 알 수 없음) 대리자 역할
- 검증집합은 테스트집합 대리자 역할
- MSE, log-likelihood 등의 손실함수는 주어진 과업의 학습 성능(=판단 기준) 대리자 역할
<br>
<br>

## 목적함수: 교차 엔트로피와 로그우도
- **평균제곱 오차**를 다시 생각하기
- **교차 엔트로피** 목적함수
- **소프트맥스**함수와 **로그우도** 목적함수
<br>
<br>

## 평균제곱 오차 다시 생각하기
- **평균제곱 오차(MSE)** 목적함수
    - $e = {1 \above 1pt 2} \left \| y - o \right \|_{2}^{2}$
    - L2 norm으로 실제값-예측값 정량화함!
    - 오차가 클수록 $e$값이 크므로 벌점(정량적 성능)으로 활용됨
    - 벌점이 크면 현재 학습이 많이 개선되어야 함!
- 하지만, 큰 **허점**이 존재
    - $e_1 = 0.2815$, $e_2 = 0.4971$일 때
    - $e_2 = 0.4971$이 더 크기 때문에, 더 큰 벌점을 받아야 마땅함.
    - 신경망 학습 과정에서 학습은 오류를 줄이는 방향으로 가중치와 편향을 교정 <- 큰 교정이 필요함에도 작은 경사도로 작게 갱신됨
    - 경사도를 계산해봤을때 $e_1$가 나온 쪽의 출력값 이전의 경사도가 $e_2$가 나온 쪽의 출력값 이전의 경사도보다 더 크면 -> 더 많은 오류를 범한 상황이 더 낮은 벌점을 받은 꼴 -> 학습이 더딘 부정적 효과
- **이유**
    - 로지스틱 시그모이드를 사용하는 경우, 로지스틱 시그모이드 함수의 도함수 그래프를 보면 $wx+b$(활성함수 그래프의 가로축)가 커지면 경사도가 작아짐. 값이 클수록 더 작은 그래디언트 값을 얻음.
    - 따라서 큰 벌점을 부과하더라도 이에 대한 갱신이 더딤.
<br>

위의 문제를 해결하기 위해서는 첫번째, 활성함수를 ReLU로 바꾼다. 두번째, MSE를 다른 목적함수로 바꾼다. 바로 교차 엔트로피로!
<br>

## 교차 엔트로피 목적함수
- **교차 엔트로피**(cross entropy)
    - 정담(label)에 해당하는 y가 확률변수(2진분류로 0 or 1이라고 가정하자)
    - 확률 분포 : $P$는 정답, $Q$ 신경망(예측) 출력
    - 확률분포를 통일된 수식으로 쓰면,
        - $P(0)=1-y, Q(0)=1-o$
        - $P(1)=y, Q(1)=o$
    - **교차 엔트로피** $H(P,Q)=-\Sigma_x P(x)log_2Q(x) = -\Sigma_{i=1,...,k}P(e_i)log_2Q(e_i)$ 적용
        - $H(P, Q) = -\Sigma_{y\in {0,1}}P(y)log_2Q(y)$
<br>

- **교차 엔트로피 목적함수**
    - $e = -(ylog_2o + (1-y)log_2(1-o))$ 이 때, $o = \sigma(z)$이고 $z = wx + b$
    - 역할을 잘 수행하는지 확인
        - y가 1, o가 0.98일 때(예측이 잘된 경우)
            - 오류 $e = -(1 log_2 0.98 + (1-1)log_2(1-0.98))=0.0291$ 로서 낮은 값
        - y가 1, o가 0.00001일 때(예측이 잘못된 경우, 혹은 오분류된 경우)
            - 오류 $e = -(1 log_2 0.00001 + (1-1)log_2(1-0.00001))=13.2877$로서 높은 값
<br>

- 위 식을 c개의 출력 노드를 가진 경우로 확장
    - 출력 벡터 $o = (o_1, o_2, ..., o_c)^T$인 상황으로 확장
        - $e = 0\sum_{i=1,c}(y_ilog_2o_i + (1-y_i)log_2(1-o_i))$
<br>
<br>

## 소프트맥스 활성함수와 로그우도 목적함수
- **소프트맥스 함수**
    - $o_j = {e^{sj} \above 1pt \Sigma_{i=1,c}e^{si}}$
    - 동작 예
        - 소프트맥스는 최대를 모방 <- 출력 노드인 중간 계산 결과 $s_iL$의 최댓값을 더욱 활성화하고 다른 작은 값들은 억제
        - 모두 더하면 1이 되어 확률 모방
<br>

- **음의 로그우도** 목적함수
    - $e = -log_2o_y$
    - 모든 출력 노드값을 사용하는 MSE나 교차 엔트로피와 달리 $o_y$라는 **하나의 노드만 적용**
    - $o_y$는 샘플의 정답에 해당하는 노드의 출력값
- **소프트맥스와 로그우도**
    - **소프트맥스는 최댓값이 아닌 값을 억제하여 0에 가깝게 만든다**는 의도 내포
    - 신경망에 의한 샘플의 정답에 **해당하는 노드만 보겠다는 로그우도**와 잘 어울림
    - 따라서 **둘을 결합하여 사용하는 경우가 많음**
- 소프트맥스와 교차 엔트로피 목적함수
    - 로그우도 손실함수 ~ 교차엔트로피 최소화
<br>

- 소프트맥스 분류기
    - 다항 로지스틱 회귀분석의 예
    - 분류기의 최종 값을 확률로 표현
    - 소프트맥스와 로그우도 목적함수