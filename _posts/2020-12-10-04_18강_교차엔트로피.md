---
title : "[인공지능 수학 - 통계학]18강 : 교차엔트로피"
data : 2020-12-10 00:15:28 -0400
categories : 프로그래머스인공지능스쿨
use_math: true
---
# 엔트로피
## 자기정보(Self-information): i(A)
- A : 사건
- $i(A) = log_b({1 \above 1pt P(A)}) = -log_bP(A)$
- 확률이 높은 사건:
    - 정보가 많지 않음(확률이 낮은 사건일수록 정보가 많음)
    - 예) 도둑이 들었는데 개가 짖는 경우보다 도둑이 들었는데 개가 안 짖는 경우가 더 많은 정보를 포함함
- 정보의 단위
    - $b = 2$ : bits
    - $b = e$ : nats
    - $b = 10$ : hartleys
- 특성
    - $i(AB) = log_b({1 \above 1pt P(A)P(B)}) = log_b({1 \above 1pt P(A)}) + log_b({1 \above 1pt P(B)}) = i(A) + i(B)$
- $P(H) = {1 \above 1pt 8}$, $P(T) = {7 \above 1pt 8}$
    - $i(H) = 3$비트, $i(T) = 0.193$비트
H는 head의 약자, T는 tail의 약자  
<br>
<br>

## 엔트로피(entropy)
- 자기 정보의 평균
    - $H(X) = \sum_j P(A_j)i(A_j) = - \sum_j P(A_j) log_2 P(A_j)$
- 특성
    - $0 \leq H(X) \leq log_2K$,
        - $K$ : 사건의 수
확률($P(A_j)$)가 $1/K$일 때(균등한 값을 가질 때) 엔트로피가 가장 큰 값을 가진다.  
<br>
<br>

- 엔트로피의 활용
    - 평균비트수를 표현
    - 데이터 압축에 사용
- 4가지 정보를 압축하는데 필요한 비트수
    - 일반적으로 2비트

<br>
<br>
<br>

# 교차엔트로피
## 확률분포 P와 Q
- $S = {A_j}$
    - $P(A_j)$ : 확률분포 P에서 사건 $A_j$가 발생할 확률
    - $Q(A_j)$ : 확률분포 Q에서 사건 $A_j$가 발생할 확률
    - $i(A_j)$ : 확률분포 Q에서 사건 $A_j$의 자기정보
        - $i(A_j) = -log_2 Q(A_j)$
        - 자기 정보는 $A_j$를 표현하는 비트수
        - 잘못된 확률분포 Q를 사용하게 되면, 실제 최적의 비트수를 사용하지 못하게 됨
<br>
<br>

## $H(P, Q)$ : 교차엔트로피
- 집합 S상에서 확률분포 P에 대한 확률분포 Q의 교차 엔트로피
- 확률분포 P에서 $i(A_j)$의 평균
    - $H(P, Q) = \sum_j P(A_j)i(A_j) = - \sum_j P(A_j) log_2 Q(A_j) = - \sum_{x \in X} P(X) log_2 Q(x)$
    - 이 값은 정확한 확률분포 P를 사용했을 때의 비트수보다 크게 됨
        - $H(P, Q) = - \sum_{x \in X} P(X) log_2 Q(x) \geq - \sum_{x \in X} P(X) log_2 P(x) = H(P)$
- 따라서 이 값은 P와 Q가 얼마나 비슷한지를 표현
    - 같으면 $H(P, Q) = H(P)$
    - 다르면 $H(P, Q) > H(P)$
교차엔트로피를 사용하면 P라는 확률분포와 Q라는 확률분포가 얼마나 비슷한지 알 수 있다.  
<br>
<br>

## 손실함수
- 분류 문제에서의 손실함수
    - 분류문제
        - 주어진 대상이 A인지 아닌지를 판단
        - 주어진 대상이 A, B, C, ...중 어느 것인지를 판단
    - 기계학습에서는 주어진 대상이 각 그룹에 속할 확률을 제공
        - 예)
            - [0.8, 0.2]: A일 확률 0.8, B일 확률 0.2
        - 이 값이 정답인 [1.0, 0.0]과 얼마나 다른지 측정 필요 -> 이것을 측정하는 것이 손실함수!
    - 원하는답 P, 제시된 답 Q가 있을 때 P와 Q가 얼마나 다른지에 대한 척도 필요
<br>
<br>

- 제곱합
    - $\sum (p_i - q_i)^2$
        - $(1.0 - 0.8)^2 + (0.0 - 0.2)^2$
    - 확률이 다를수록 큰 값을 가짐
    - 하지만 학습 속도 느림
- 교차 엔트로피 $H(P, Q)$:
    - 확률이 다를수록 큰 값을 가짐
    - 학습 속도 빠름
    - 분류 문제에서 주로 교차 엔트로피 사용
<br>
<br>

- 참고 : 분류 문제에서의 원하는 답
- $P = [p_1, p_2, ..., p_n]$
    - $p_i$중 하나만 1이고 나머지는 다 0임
        - 엔트로피는 0, 즉 $H(P) = 0$
    - $p_k = 1.0$이라고 하면, $q_k$의 값이 최대한 커지는 방향으로 학습 진행
<br>
<br>

- S = {A, B}
    - 실제상황
        - P = [1, 0]
            - P(A) = 1, P(B) = 0
    - 예측 Q(X)
        - [0.8, 0.2] : Q(A) = 0.8, Q(B) = 0.2
            - $H(P, Q) = - \sum_{x \in X} P(x) log_2 Q(x) = -1 * log_2 0.8 = 0.3219$
        - [0.2, 0.8] : Q(A) = 0.8, Q(B) = 0.2
            - $H(P, Q) = - \sum_{x \in X} P(x) log_2 Q(x) = -1 * log_2 0.2 = 2.32$
내가 원하는 값과 다른 값이 나오면 교차엔트로피 값이 커진다.(확률이 다를수록 큰 값을 가짐)  
위와 같이 두가지 문제(경우) 뿐만 아니라 4가지 문제 등 다양한 경우에서 사용 가능하다. 바로 아래처럼!  
```python
import numpy as np
def crossentropy(P,Q):
    return sum([-P[i]*np.log2(Q[i]) for i in range(len(P))])
P = [1, 0, 0, 0]
Q = [0.7, 0.1, 0.1, 0.1]
print(crossentropy(P,Q))
```
교차엔트로피를 인공지능, 기계학습에서 손실함수로 주로 사용한다.  