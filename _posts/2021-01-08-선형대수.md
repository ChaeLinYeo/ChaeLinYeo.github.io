---
title : "[TIL]선형대수"
data : 2021-01-08 00:15:28 -0400
categories : 프로그래머스인공지능스쿨
use_math: true
---
원래 블로그에 인공지능 관련 공부 내용을 정리할 때 오늘 배운 것을 전부 기록하는 방식으로 하고 있었다. 배운 내용은 하나도 빠짐없이 남기고 싶었고, 학교에서도 수업을 들을때 교수님의 말씀을 하나하나 다 기록하는 것이 습관이 되어 있었기 때문에 너무 욕심을 내서 기록을 진행했던 것 같다.  
물론 성실하고 꼼꼼하게 다 기록을 남기는 것은 좋지만, 하루에 듣는 강의량과 공부량이 많기 때문에 "기록"을 님기는 것에 집중하기 보단 나만의 "공부"에 좀더 초점을 맞춰야겠다는 생각이 들었다.  
배움을 기록하는 것에 대해 오해를 갖고있었던 것 같다. 오늘 배운 것을 다 정리하는 것은 나를 위한 기록이라면, 배움 기록은 '공개'된 곳에 '정리'하는 것이라고 생각한다.  
그래서 오늘부터는 TIL로 배운 내용과 생각을 정리하기로 했다.  
이런 "생각"에 대한 인사이트가 나중에 많이 도움이 될 것이라 생각했기 때문이다.  
물론, 지금까지 했던 것처럼 배운 내용은 나만의 기록으로, 원노트를 이용해 성실히 따로 기록할 것이다.  

## 왜 선형대수를 알아야 하는가?
Deep learning을 이해하기 위해서 반드시 선형대수 + 행렬미분 + 확률의 탄탄한 기초가 필요하다.  
예) Transformer의 attention matrix:
$\mathrm{Att}_{\leftrightarrow}(Q, K, V) = D^{-1}AV, ~A = \exp(QK^T/\sqrt{d}), ~D = \mathrm{diag}(A1_L)$  
이렇게 핵심 아이디어가 행렬에 관한 식으로 표현되는 경우가 많다.  
선형대수와 행렬미분의 기초를 배우고 간단한 머신러닝 알고리즘(PCA)을 유도해보았다.  
<br>
<br>

- 선형대수의 기본 표기법
- 행렬의 곱셈
    - 벡터 * 벡터
    - 행렬 * 벡터
    - 행렬 * 행렬
- 중요 연산과 성질들
    - 정방, 삼각, 대각, 단위행렬의 성질
- 전치(transpose)
- 대각합(trace)
- 선형독립과 Rank
- 역행렬
- 직교 행렬(Orthogonal Matrices)
- 치역(Range), 영공간(Nullspace)
- 행렬식(Determinant)
- 이차형식(Quadratic Forms)
- 고유값(Eigenvalues), 고유벡터(Eigenvectors), 대칭행렬
- 행렬미분
    - The Gradient
    - The Hessian
    - 최소제곱법(Least Squares)
    - 고유값과 최적화문제(Eigenvalues as Optimization)
- Autoencoder와 Principal Components Analysis(PCA)
    - 디코딩 함수
    - 인코딩 함수
    - 최적의 D 찾기
<br>

선형대수는 예전에 학교 수업으로 배웠던 내용이다. 새록새록 기억나면서도 기억나지 않는 부분도 많아서 이전 필기를 찾아보면서 복습해야겠다. 