---
title : "[Machine Learning 기초]선형회귀"
data : 2021-01-06 00:15:28 -0400
categories : 프로그래머스인공지능스쿨
use_math: true
---
## 선형회귀 (Linear Regression)
먼저, 주어진 데이터를 직선을 사용해 모델링하는 방법을 살펴본다. 직선함수는 다음과 같은 형태를 가진다.  
$
y = ax + b
$  
여기서 $a$는 기울기 (*slope*)이고 $b$는 $y$절편 (*intercept*)라고 불린다.  
Scikit-Learn의 ``LinearRegression`` estimator를 사용해서 데이터를 가장 잘 표현하는 직선을 찾을 수 있다.  
<br>
<br>

## 선형 기저함수 모델 (Linear Basis Function Models)
비선형데이터를 선형함수로 모델링하는 한가지 방법은 기저함수(basis function)을 사용하는 것이다.  
예를 들어, 다음과 같은 선형함수를 사용한다고 하자.  
$
y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \cdots
$  
여기서 $x_1, x_2, x_3,$등을 1차원 $x$로부터 생성할 수 있다($x_n = f_n(x)$). $f_n$을 기저함수(basis function)라고 부른다.  
만약 $f_n(x) = x^n$라는 기저함수를 사용하면 최종적인 모델은 다음과 같을 것이다.  
$
y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \cdots
$  
이 모델은 여전히 계수($a_n$)에 관해서는 선형함수임을 기억하자. 따라서 1차원 변수인 $x$를 기저함수를 통해 다차원으로 확장시킴으로써 우리는 여전히 선형모델(linear regression)을 사용할 수 있게 된다.  
<br>
<br>

### 다항 기저함수 (Polynomial Basis Functions)
$f_n(x) = x^n$ 형태의 함수를 다항 기저함수 (polynomial basis functions)라고 부른다. Scikit-Learn은 ``PolynomialFeatures``이라는 transformer를 이미 포함하고 있다.### 다항 기저함수 (Polynomial Basis Functions)  
$f_n(x) = x^n$ 형태의 함수를 다항 기저함수 (polynomial basis functions)라고 부른다. Scikit-Learn은 ``PolynomialFeatures``이라는 transformer를 이미 포함하고 있다.  
<br>
<br>

### 가우시안 기저함수 (Gaussian Basis Functions)
다항 기저함수 외에 다른 기저함수를 사용해보자. 가우시안 기저함수는 다음과 같이 정의된다.  
$\exp \{-\frac{(x-u_j)^2}{2s^2}\}$  
$u_j$는 함수의 위치, $s$는 폭을 결정한다. 주어진 데이터를 여러 개의 가우시안 기저함수들의 합으로 표현하려고 시도할 수 있다.  
가우시안 기저함수는 Scikit-Learn에 포함되어 있지 않다.  
<br>
<br>

## 규제화 (Regularization)
기저함수를 사용함으로써 복잡한 데이터를 모델링할 수 있게 되었지만 조심하지 않는다면 over-fitting이라는 다른 심각한 문제를 만날 수 있다! 예를 들어, 너무 많은 개수의 가우시안 기저함수를 사용하게 되면 모델이 필요이상으로 flexible해져서 데이터가 없는 곳에서는 극단적인 값을 가질 수 있다.  
Over-fitting이 일어나는 영역에서는 인접한 기저함수들의 값이 극단으로 가면서 서로 상쇄하는 현상이 일어난다. 따라서 큰 계수값에 대해 penalty를 부여해서 over-fitting을 어느 정도 극복할 수 있을 것이다. 이러한 penalty를 *regularization*이라 부른다.  
<br>
<br>

### Ridge regression ($L_2$ Regularization)
가장 자주 쓰이는 형태의 regularization은 *ridge regression* ($L_2$ *regularization*)이고 다음과 같이 정의된다.  
$
P = \alpha\sum_{n=1}^N \theta_n^2
$  
여기서 $\alpha$는 regularization의 강도를 조절하는 파라미터이다. 이 형태의 regularization은 Scikit-Learn의 ``Ridge`` estimator에서 사용된다.  
$\alpha$값이 0에 가까워질수록 일반적인 선형회귀모델이 되고 $\alpha$값이 무한대로 증가하면 데이터는 모델에 영향을 주지 않게 된다.  
<br>
<br>

### Lasso regression ($L_1$ regularization)
또 하나의 자주 쓰이는 regularization방법은 계수들의 절대값의 합을 제한하는 것이다.  
$
P = \alpha\sum_{n=1}^N |\theta_n|
$  
뒤에서 자세히 다루겠지만 이 방법은 *sparse*한 모델을 생성하게 된다(많은 계수들이 0이 됨).  
Ridge regression과 마찬가지로 $\alpha$값으로 regularization의 강도를 조절할 수 있다.  