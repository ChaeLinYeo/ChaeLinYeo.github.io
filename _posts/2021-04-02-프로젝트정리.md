---
title : "Project Troubleshooting"
data : 2021-04-02 00:15:28 -0400
categories : 프로그래머스인공지능스쿨
use_math: true
---
## Detectron2란?
[https://chacha95.github.io/2020-04-28-Object-Detection5/](https://chacha95.github.io/2020-04-28-Object-Detection5/)
Detectron2는 Facebook에서 만든 pytorch기반 object detection과 sementic segmentation 알고리즘을 구현하는 시스템이다. 연산량이 많이 소요되는 부분을 python이 아닌 CUDA와 C로 구현해 보다 빠르고 좋은 성능을 낼 수 있다. 다양한 object detection 알고리즘과 pretrained model zoo를 제공하기 때문에 pretraining에 대한 부담을 줄이고 training/inference에 집중할 수 있었다. 이러한 장점들 때문인지 논문 구현에도 널리 쓰인다.  
pretraining에 대한 부담은 덜었지만 validation loss score계산, best model 추출, F-score 계산 등에 있어 커스텀 구현이 필요했다. 이 부분에 대해서는 뒤에서 코드와 함께 자세히 다룬다.  
처음에는 Detectron2의 사용방법을 익히고 뼈대 코드를 살펴보기 위해 [튜토리얼 코드](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)를 참고했다.  
Detectron2 튜토리얼 기반으로 Dataset Loading / Train Model / Using Model for Inference 네 부분으로 나눠 함수를 구현했다. 이 중에서도 Train Model / Using Model for Inference부분을 만들었다.  
<br>
<br>

## 모델 선정
object detection을 위한 여러 모델 후보들이 있었다.  
- Faster R-CNN ✅
- Cascade R-CNN ✅
- YoloV3
- DETR
- EfficientDet
모델 선정에 있어 멘토님과 여러번 질의응답을 주고받았다. 모델을 고르는데 있어 비교적 최신 모델을 써보고 싶은 욕심이 있었고, 또 학습이 용이하고 최적의 성능을 낼 수 있는 모델이어야 했다. 우리 프로젝트의 경우 두개의 시나리오로 나눠서 구현하는데,  
1. 첫번째 시나리오: 카메라가 있는 모바일 디바이스 사용 시 사용자의 얼굴 인식 마스크 착용여부를 판단하여 미착용시 알람 및 사용자의 현재 위치에서 가장 가까운 약국/편의점 안내
2. 두번째 시나리오: CCTV영상에서 사람들의 마스크 착용여부를 인식하여 일정 시간 간격으로 미착용 인원이 있을 시 마스크 착용 안내방송 송출
두번째 시나리오의 경우 실시간성도 중요하다고 생각해서, Real time일 경우 가장 많이 쓰이는 Yolo를 쓸까 생각했는데 Detectron에서는 Yolo를 지원하지 않는다.  
DETR과 EfficientDet는 2020년에 나온 최근의 object detection 모델이다. 논문에서는 기존의 모델들보다 더 좋은 성능을 자랑하지만 튜닝하기가 어렵고, 경우에 따라 잘 안될 수 있다고 한다.  
최근에 나온 모델은 아니지만 object detection 역사에 있어 가장 유명한 모델 중 하나인 Faster R-CNN과 Cascade R-CNN을 찾아보았다. 디텍트론 튜토리얼을 활용해 위의 두 모델과 DETR을 돌려보았고, 멘토님의 말씀을 참고하여 fine tuning할 수 있고 자료가 많은 Faster R-CNN과 Faster R-CNN을 선택했다. 그중에서도 나는 Faster R-CNN으로 모델 학습을 맡았다.  
<br>
<br>

## Colab 환경에서 Detectron2 설치 및 환경 세팅
### Install dependencies
⚠️ Detectron2는 기본적으로 리눅스 환경에서 실행할 수 있다. MacOS에서도 설치할 수 있으나 CUDA를 쓸 수 없어 결국 Colab을 사용했다.  
⚠️ 반드시 Restart Runtime을 해줄 것!  
프로젝트 후반에 torch 1.8버전이 사용 가능해졌다.  
```python
  install dependencies: 
!pip install pyyaml==5.1
!pip install torch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
!gcc --version
# opencv is pre-installed on colab
# install detectron2: (Colab has CUDA 10.1 + torch 1.7)
# See https://detectron2.readthedocs.io/tutorials/install.html for instructions
import torch
assert torch.__version__.startswith("1.7")
!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html
# exit(0)  # After installation, you need to "restart runtime" in Colab. This line can also restart runtime
```
<br>
<br>

### Google drive mount
마스크 데이터셋이 있는 gdrive 계정을 마운트한다.  
⚠️ 데이터셋 폴더를 공유받아 사용할 경우 공유문서함에서 해당 폴더 우클릭 > 드라이브에 바로가기 추가 > 내 드라이브 를 선택해야 colab에서 접근이 가능하다.  
⚠️ force_remount=True 옵션을 꼭 줘야하고, 폴더 공유는 원 소유자가 해야한다.  
⚠️ 정확한 이유는 모르지만, gmail 계정이 아니지만 gdrive가 있는(학교메일 계정 등) 계정으로 공유받으면 drive mount도 잘 되고 폴더에 파일 접근도 가능한데, File not found error, json decode error, path에 리스트가 들어와야하는데 문자열이 들어았다 등의 오류가 발생한다. 이 문제로 열심히 코드를 고쳤었는데 코드의 문제가 아닌 구글 계정의 문제였다.  
```python
from google.colab import drive
drive.mount('/gdrive', force_remount=True)
```
<br>
<br>

## 학습을 위한 준비
### 디렉토리 구성/파일 로딩
학습을 이어서 할 때 colab의 런타임이 끊기면 다음과 같이 필요한 디렉토리들을 생성하고 gdrive에서 학습에 필요한 pth와 json 파일을 가져온다.  
pth, json파일을 args에서 지정한 output 경로에 `model_final.pth`와 `metrics.json`이라는 이름으로 저장되었던 파일이다. 매번 학습할 때마다 gdrive에 저장해두고 이어서 학습이 필요한 부분을 가져와서 쓰곤 했다.  
```python
import os.path as osp
from detectron2.config import get_cfg
cfg = get_cfg()
tensorboard_dir = osp.join(cfg.OUTPUT_DIR, 'tensorboard')
checkpoint_dir = osp.join(cfg.OUTPUT_DIR, 'checkpoints')
os.makedirs("coco_eval", exist_ok=True)
os.makedirs(tensorboard_dir, exist_ok=True)
os.makedirs(checkpoint_dir, exist_ok=True)
path = os.path.join(cfg.OUTPUT_DIR, "inference")
os.makedirs(path, exist_ok=True)
```

```python
import shutil
shutil.copy2('/gdrive/MyDrive/Datasets/faster_rcnn_model3_pth/faster_rcnn_model_val_AP_30000iter_210325.pth', '/content/output/model_final.pth')
shutil.copy2('/gdrive/MyDrive/Datasets/faster_rcnn_model3_pth/faster_rcnn_model_val_AP_30000iter__210325.json', '/content/output/metrics.json')
```
<br>
<br>

### 수정가능한 파라미터들 설정하기
⚠️ floating error 발생 시 learning rate를 약 0.000001 정도로 굉장히 작게 주면 해결된다.  
모델 훈련에 사용될 cfg 구성을 위한 파라미터들 지정하기.  
- `gpu` : 보통 0으로 지정. 첫번째 gpu 사용
- `resume` : 훈련한 pth를 이어받아서 훈련할지의 여부(True/False)
- `output` : 훈련 결과가 저장될 디렉토리 위치
- `max_iter` : max iteration 지정
- `num_machines` : the total number of machines, 1로 설정
- `learning_rate` : 학습률 지정
- `ims_per_batch` : gpu 수 * 2
- `batch_size_per_image` : 훈련을 위해 샘플링할 이미지 수
- `num_classes` : 데이터셋의 분류 클래스 수(여기서는 with mask/no mask로 2개)
- `eval_period` : evaluation이 시행되는 period
- `mask_on` : cascade r-cnn을 사용할 경우 True, faster r-cnn을 사용할 경우 False

```python
def parse_args():
    """
    Specifying parameters to configure cfg
    @return: arguments for modifying cfg
    """
    args = easydict.EasyDict({
        "gpu": 0,
        "resume": True,
        "output": "/content/output/",
        "max_iter": 300,
        "num_machines": 1,
        "eval_only": False,
        "learning_rate": 0.0005,
        "ims_per_batch": 2,
        "batch_size_per_image": 512,
        "num_classes": 2,
        "eval_period": 100,
        "mask_on": False
    })
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    return args
```
<br>
<br>

### 파라미터를 받아 모델 학습을 위한 configuration 구성하기
⚠️ colab 런타임이 재설정되면 resume=True로 이전 pth을 이어 학습할 때 iteration이 0부터 시작하는 문제가 있었다. `cfg.MODEL.WEIGHTS = "/content/output/model_final.pth"` 로 이전에 학습한 모델의 weight를 넣어주고, colab content 디렉토리에 자동 생성되는 last_checkpoint 텍스트 파일 안에 이어받을 weight 파일 이름을 적어주면 iteration과 pth를 모두 이어받아 학습이 가능하다.  
⚠️ pth 파일은 디폴트로 5000iteration에 한번씩 저장이 된다.  
- `args`: configuration을 수정하기 위한 파라미터들. `parse_args()` 를 통해 생성된다.
- `cfg.merge_from_file` : model_zoo에서 사용할 모델의 `.yaml` 파일 경로를 지정한다.
- `cfg.MODEL.WEIGHTS` : 처음 학습할 경우 model_zoo에서 사용할 모델의 `.yaml` 경로를 가져오고, resume=True로 이어서 학습할 경우 이전에 학습되었던 `model_final.pth` 의 경로를 지정한다.
- `cfg.MODEL.MASK_ON` : cascade r-cnn의 경우 mask의 유무를 위한 True/False 값
- `cfg.DATASETS.TRAIN` : register된 데이터셋 중 학습에 사용할 데이터셋의 이름
- `cfg.DATASETS.TEST` : register된 데이터셋 중 테스트에 사용할 데이터셋의 이름
- `cfg.SOLVER.BASE_LR` : 모델 학습에 적용될 학습률
- `cfg.SOLVER.IMS_PER_BATCH` : gpu수 * 2
- `cfg.SOLVER.MAX_ITER` : max iteration 수
- `cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE` : 훈련을 위해 샘플링할 이미지 수
- `cfg.MODEL.ROI_HEADS.NUM_CLASSES` : 데이터셋의 분류 클래스 수
- `cfg.MY_CUSTOM.LOG_FILE` : Epoch별 iteration, best loss, total loss가 출력되는 'my_log.txt'가 저장되는 디렉토리 경로
- `cfg.OUTPUT_DIR` : 훈련 결과가 저장될 디렉토리 위치
- `cfg.TEST.EVAL_PERIOD` : evaluation이 시행되는 period

```python
def setup(args):
    """
    Create configs and perform basic setups.
    @param args: arguments for modifying configuration
    @return: cfg for model training
    """
    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")
    # cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, 'model_final.pth')
    cfg.MODEL.MASK_ON = args.mask_on
    cfg.DATASETS.TRAIN = ('mask_train',)
    cfg.DATASETS.TEST = ('mask_val',)
    cfg.SOLVER.BASE_LR = args.learning_rate
    cfg.SOLVER.IMS_PER_BATCH = args.ims_per_batch
    cfg.SOLVER.MAX_ITER = args.max_iter
    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = args.batch_size_per_image
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = args.num_classes
    cfg.MY_CUSTOM.LOG_FILE = os.path.join(cfg.OUTPUT_DIR, 'my_log.txt')
    cfg.OUTPUT_DIR = args.output
    cfg.TEST.EVAL_PERIOD = args.eval_period
    # cfg.freeze()
    return cfg
```
<br>
<br>

### 데이터셋 등록하기
train, validation 데이터셋의 annotation이 담긴 csv를 register로 등록한다.  

```python
ltd = Load_Train_Data(DIR_INPUT+'train_2.csv', DIR_TRAIN)
clear_dataset_catalog()
register_dataset_catalog(ltd, phase=['mask_train'], classes=['with', 'No'])
vtd = Load_Train_Data(DIR_INPUT+'val_2.csv', DIR_TRAIN)
register_dataset_catalog(vtd, phase=['mask_val'], classes=['with', 'No'])
mask_metadata = MetadataCatalog.get("mask_train").set(thing_classes=['with', 'No'], evaluator_type="coco")
```
<br>
<br>

### Dataset dictionary 만들기
표준 데이터 세트 dictionary 형식을 따르는 dictionary 목록을 반환  
- `dataset_names`: a list of dataset names
- `filter_empty`: whether to filter out images without instance annotations
- `param min_keypoints`: filter out images with fewer keypoints than `min_keypoints`. Set to 0 to do nothing.
- `proposal_files`: if given, a list of `objeget_facemask_1_dictsct` proposal files that match each dataset in `dataset_names`.

```python
def get_detection_dataset_dicts(dataset_names, filter_empty=True, min_keypoints=0, proposal_files=None):
    """
    Load and prepare dataset dicts for instance detection/segmentation and semantic segmentation.
    @param dataset_names: a list of dataset names
    @param filter_empty: whether to filter out images without instance annotations
    @param min_keypoints: filter out images with fewer keypoints than `min_keypoints`. Set to 0 to do nothing.
    @param proposal_files: if given, a list of objeget_facemask_1_dictsct proposal files that match each dataset in `dataset_names`.
    @return: a list of dicts following the standard dataset dict format.
    """
		assert len(dataset_names)
    dataset_dicts = [DatasetCatalog.get(dataset_name) for dataset_name in dataset_names]
    for dataset_name, dicts in zip(dataset_names, dataset_dicts):
        assert len(dicts), "Dataset '{}' is empty!".format(dataset_name)

    if proposal_files is not None:
        assert len(dataset_names) == len(proposal_files)
        # load precomputed proposals from proposal files
        dataset_dicts = [
            load_proposals_into_dataset(dataset_i_dicts, proposal_file)
            for dataset_i_dicts, proposal_file in zip(dataset_dicts, proposal_files)
        ]

    dataset_dicts = list(itertools.chain.from_iterable(dataset_dicts))

    has_instances = "annotations" in dataset_dicts[0]
    if filter_empty and has_instances:
        dataset_dicts = filter_images_with_only_crowd_annotations(dataset_dicts)
    if min_keypoints > 0 and has_instances:
        dataset_dicts = filter_images_with_few_keypoints(dataset_dicts, min_keypoints)

    if has_instances:
        try:
            check_metadata_consistency("thing_classes", dataset_names)
        except AttributeError:  # class names are not available for this dataset
            pass
    return dataset_dicts
```
<br>
<br>

### validation dataloader 만들기
- `cfg`: configuration for model training
- `dataset_name`: validation dataset name
- `mapper`: DatasetMapper
```python
def build_detection_val_loader(cfg, dataset_name: str, mapper=None):
    """
    @param cfg: configuration for model training
    @param dataset_name: validation dataset name
    @param mapper: DatasetMapper
    @return:
    """
		dataset_dicts = get_detection_dataset_dicts(
        [dataset_name],
        filter_empty=False,
        proposal_files=[
            cfg.DATASETS.PROPOSAL_FILES_TEST[list(cfg.DATASETS.TEST).index(dataset_name)]
        ]
        if cfg.MODEL.LOAD_PROPOSALS
        else None,
    )
    dataset = DatasetFromList(dataset_dicts)

    if mapper is None:
        mapper = DatasetMapper(cfg, True)
    dataset = MapDataset(dataset, mapper)
    sampler = InferenceSampler(len(dataset))
    batch_sampler = torch.utils.data.sampler.BatchSampler(sampler, 1, drop_last=False)

    data_loader = torch.utils.data.DataLoader(
        dataset,
        num_workers=cfg.DATALOADER.NUM_WORKERS,
        batch_sampler=batch_sampler,
    )
    return data_loader
```
<br>
<br>

### 체크포인트 만들기
best loss 모델이 저장될 체크포인트를 만든다.  
```python
class AdetCheckpointer(DetectionCheckpointer):
    def _load_file(self, filename):
				"""
        load native pth checkpoint
        @param filename:
        @return:
        """
        if filename.endswith(".pkl"):
            with PathManager.open(filename, "rb") as f:
                data = pickle.load(f, encoding="latin1")
            if "model" in data and "__author__" in data:
                # file is in Detectron2 model zoo format
                self.logger.info("Reading a file from '{}'".format(data["__author__"]))
                return data
            else:
                # assume file is from Caffe2 / Detectron1 model zoo
                if "blobs" in data:
                    # Detection models have "blobs", but ImageNet models don't
                    data = data["blobs"]
                data = {k: v for k, v in data.items() if not k.endswith("_momentum")}
                return {"model": data, "__author__": "Caffe2", "matching_heuristics": True}

        loaded = super()._load_file(filename)  # load native pth checkpoint
        if "model" not in loaded:
            loaded = {"model": loaded}
        if "lpf" in filename:
            loaded["matching_heuristics"] = True
        return loaded
```
<br>
<br>

### 모델 evaluator
주어진 데이터셋에 대해 evaluator를 생성한다. 여기서는 bounding box detection을 지원하는 COCOEvaluator를 사용했다.  

- `cfg` : 모델 학습을 위한 configuration
- `dataset_name` : `cfg.DATASETS.TEST` 에 지정한 데이터셋을 가져온다
- `output_folder` : evaluator의 결과 json파일이 저장될 `inference` 디렉토리

```python
def get_evaluator(cfg, dataset_name, output_folder=None):
	"""
    Create evaluator(s) for a given dataset.
    This uses the special metadata "evaluator_type" associated with each builtin dataset.
    For your own dataset, you can simply create an evaluator manually in your
    script and do not have to worry about the hacky if-else logic here.
    @param cfg: configuration for model training
    @param dataset_name: Get the data set specified in cfg.DATASETS.TEST
    @param output_folder: Inference directory where the resulting json file of evaluator will be saved
    @return:
    """
    if output_folder is None:
        output_folder = os.path.join(cfg.OUTPUT_DIR, "inference")
    evaluator_list = []
    evaluator_list.append(COCOEvaluator(dataset_name, cfg, True, output_folder))
    if len(evaluator_list) == 1:
        return evaluator_list[0]
    return DatasetEvaluators(evaluator_list)
```
<br>
<br>

### 모델 validation
⚠️ Detectron2에서는 옵션으로 별도의 validation loss를 계산해주지 않는다. 직접 구현해야 했다.  
- `cfg` : 모델 훈련을 위한 configuration
- `model` : `build_model`을 사용하여 모델 구조를 빌드하는 모델
- `val_dataloader` : 학습 단계에서 `build_detection_val_loader` 을 통해 생성되는 validation dataloader  

```python
def do_val(cfg, model, val_dataloader):
    """
    @param cfg: configuration for model training
    @param model: model that builds the model structure with `build_model`
    @param val_dataloader: Validation dataloader generated through build_detection_val_loader in the learning phase
    @return:
    """
		with torch.no_grad():
        losses = []
        tmp = None
        for idx, inputs in tqdm(enumerate(val_dataloader)):
            outputs = model(inputs)
            loss_dict_reduced = {
                k: v.item() for k, v in comm.reduce_dict(outputs).items()
            }
            tmp = loss_dict_reduced
            reduced_loss = sum(loss for loss in loss_dict_reduced.values())
            losses.append(reduced_loss)
    val_loss = np.mean(losses)
    return val_loss
```
<br>
<br>

## 모델 훈련하기
⚠️ detectron2에서는 옵션 등으로 best loss를 측정하고 early stop하는 기능이 없어 커스텀으로 구현해야 한다.  
⚠️ 처음에는 detectron2의 공식 깃헙의 이슈의 질의응답을 참고하여 커스텀 hook와 trainer를 만들어 validation을 계산할 수 있도록 했다. 오버피팅을 시각적으로 확인할 수 있는 방법이 필요하다고 생각했고, 훈련에 validation을 포함하여 loss를 계산했다. 다음은 validation 계산과 더불어 best model을 early stop으로 구하는 과정을 포함하여 수정된 버전이다.  
⚠️  처음 코드에서는 best loss를 자동으로 저장하지 않고 iteration을 지정한 만큼 무조건 훈련을 수행했다. 10000 iteration 단위로 weight결과인 pth 파일을 저장했고, AP 및 total/validation loss 그래프  분석 결과 30000 iteration 이후부터 오버피팅이 발생하는 것을 확인할 수 있었다. 따라서 20000 iteration부터 이어서 학습을 진행하되 best loss에서 early stop하도록 했다.  
`best_loss`를 디폴트로 100으로 시작한다. iteration을 돌면서 `best_loss` 가 갱신되고 `n_early_epoch` 횟수 만큼 best loss에 진전이 없으면 early stop 한 후 모델을 저장한다.  
log.txt와 mylog.txt가 생성되는데, log.txt에는 터미널에서 출력된 모든 결과가 저장되고, mylog.txt에는 iteration마다 해당하는 best loss와 total loss, 그리고 Epoch마다 결정되는 best validation loss가 저장된다. 이를 통해 모델이 어느 시점에서 가장 최고의 성능을 보였는지 확인할 수 있다.  
훈련을 하면서 total loss와 validation loss가 metrics.json에 저장된다. 이 정보를 가지고 total/validation loss score에 대한 plot을 그릴 수 있다.  
`resume` 을 통해 이전에 학습한 pth를 이어받아 학습이 가능하다.  
- `cfg` : 모델 훈련을 위한 configuration
- `model` : `build_model`을 사용하여 모델 구조를 빌드하는 모델
- `resume` : 훈련한 pth를 이어받아서 훈련할지의 여부(True/False)
- `val_set` : validation을 위한 데이터셋

```python
def do_train(cfg, model, resume=True, val_set='mask_val'):
		"""
    train model
    @param cfg: configuration for model training
    @param model: model that builds the model structure with `build_model`
    @param resume: resume train or not
    @param val_set: dataset for validation
    """
    model.train()
    optimizer = build_optimizer(cfg, model)
    scheduler = build_lr_scheduler(cfg, optimizer)
    print_every = 50

    tensorboard_dir = osp.join(cfg.OUTPUT_DIR, 'tensorboard')
    checkpoint_dir = osp.join(cfg.OUTPUT_DIR, 'checkpoints')

    os.makedirs(tensorboard_dir, exist_ok=True)
    os.makedirs(checkpoint_dir, exist_ok=True)

    checkpointer = AdetCheckpointer(
        model, checkpoint_dir, optimizer=optimizer, scheduler=scheduler
    )
    start_iter = (
            checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get("iteration", -1) + 1
    )
    max_iter = cfg.SOLVER.MAX_ITER

    periodic_checkpointer = PeriodicCheckpointer(
        checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter
    )

    writers = (
        [
            CommonMetricPrinter(max_iter),
            TensorboardXWriter(tensorboard_dir),
        ]
        if comm.is_main_process()
        else []
    )
    data_loader = build_detection_train_loader(cfg)
    val_dataloader = build_detection_val_loader(cfg, val_set)

    logger.info("Starting training from iteration {}".format(start_iter))

    # [PHAT]: Create a log file
    log_file = open(cfg.MY_CUSTOM.LOG_FILE, 'w')

    best_loss = 100
    count_not_improve = 0
    # train_size = 6690
    # epoch_size = int(train_size/cfg.SOLVER.IMS_PER_BATCH)
    epoch_size = 100
    n_early_epoch = 10

    with EventStorage(start_iter) as storage:
        for data, iteration in zip(data_loader, range(start_iter, max_iter)):
            iteration = iteration + 1
            storage.step()
            loss_dict = model(data)
            losses = sum(loss for loss in loss_dict.values())

            assert torch.isfinite(losses).all(), loss_dict

            # Update loss dict
            loss_dict_reduced = {
                k: v.item() for k, v in comm.reduce_dict(loss_dict).items()
            }
            losses_reduced = sum(loss for loss in loss_dict_reduced.values())
            if comm.is_main_process():
                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)

            # Early stopping
            if (iteration > start_iter) and ((iteration - start_iter) % epoch_size == 0):

                val_loss = do_val(cfg, model, val_dataloader)

                if val_loss >= best_loss:
                    count_not_improve += 1
                    # stop if models doesn't improve after <n_early_epoch> epoch
                    # if count_not_improve == epoch_size*n_early_epoch:
                    if count_not_improve == n_early_epoch:
                        break
                else:
                    count_not_improve = 0
                    best_loss = val_loss
                    periodic_checkpointer.save("best_model_early")

                log_file.write(f"Epoch {(iteration - start_iter) // epoch_size}, val_loss: {val_loss}\n")
                comm.synchronize()

            optimizer.zero_grad()
            losses.backward()
            optimizer.step()

            lr = optimizer.param_groups[0]["lr"]
            storage.put_scalar("lr", lr, smoothing_hint=False)
            scheduler.step()

            if iteration - start_iter > 5 and ((iteration - start_iter) % print_every == 0 or iteration == max_iter):
                for writer in writers:
                    writer.write()
                # Write my log
                log_file.write(f"[iter {iteration}, best_loss: {best_loss}] total_loss: {losses}, lr: {lr}\n")
            periodic_checkpointer.step(iteration)
    log_file.close()
```
<br>
<br>

## 모델 테스트하기
`get_evaluator` 를 호출하여 COCOEvaluator를 이용해 평가한다. 평가 결과에 따른 AP 테이블을 확인할 수 있다.  

- `cfg` : 모델 훈련을 위한 configuration
- `model` : `build_model`을 사용하여 모델 구조를 빌드하는 모델

```python
def do_test(cfg, model):
		"""
    test model
    @param cfg: configuration for model training
    @param model: model that builds the model structure with `build_model`
    @return: test results
    """
    results = OrderedDict()
    for dataset_name in cfg.DATASETS.TEST:
        data_loader = build_detection_test_loader(cfg, dataset_name)
        evaluator = get_evaluator(
            cfg, dataset_name, os.path.join(cfg.OUTPUT_DIR, "inference", dataset_name)
        )
        results_i = inference_on_dataset(model, data_loader, evaluator)
        results[dataset_name] = results_i
        if comm.is_main_process():
            logger.info("Evaluation results for {} in csv format:".format(dataset_name))
            print_csv_format(results_i)
    if len(results) == 1:
        results = list(results.values())[0]
    return results
```

⚠️  시나리오2번에서 CCTV영상을 사용하기 때문에 추론에서 사용할 CCTV영상 데이터셋을 열심히 찾아보았으나 개인정보 문제 때문인지 구하기 어려웠다. [1프레임 단위로 사진으로 저장된 CCTV 데이터셋](https://data.mendeley.com/datasets/v3kry8gb59/1)을 찾아 추론을 돌려보았다.  

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a10f59a9-2ffe-4920-9101-fbbf4c127a4d/box_00001000.jpg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a10f59a9-2ffe-4920-9101-fbbf4c127a4d/box_00001000.jpg)

다음과 같이 비교적 가깝고 먼 얼굴들을 잘 디텍션 하는 경우도 있었지만  

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/97750813-3962-4186-a2e5-43254e80794a/box_00003330.jpg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/97750813-3962-4186-a2e5-43254e80794a/box_00003330.jpg)

어둡거나, 멀리 있는 얼굴에 대해서는 디텍션하지 못하는 경우도 많았다. 우리가 학습한 데이터셋에 작은 얼굴의 개수도 충분히 많고, 다양한 명도와 채도, 해상도를 가진 사진들이 있었지만 실제 CCTV에 대해 학습하진 않았기 때문에 이 데이터셋에 대한 학습을 따로 진행해보았다.  

해당 데이터셋에 대해 AP가 비정상적으로 낮게 나왔었는데, 데이터셋의 ground truth를 확인해보니 다음과 같이 boundary box가 얼굴 기준이 아닌 사람의 전체 형태를 기준으로 하는 것을 확인할 수 있었다. 그리고 현재 가지고 있는 데이터셋은 bounding box를 눈코입이 보이는 얼굴을 기준으로 하는데, 해당 데이터셋은 뒷모습 등 얼굴이 보이지 않는 경우도 취급하기 때문에 부적절하다고 판단했다.  

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/15bdf0de-84a8-4f50-825e-6d8d5bbbae28/example.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/15bdf0de-84a8-4f50-825e-6d8d5bbbae28/example.png)

결국 해당 데이터셋은 정성평가를 위해 사용하기로 했다. 히잡이나 지나치게 어두운 장소에서 찍힌 사진을 정교하게 디텍션하지 못하는 것은 아쉬웠지만, 30/15/10/1 프레임 단위로 데이터셋을 잘라 추론을 진행해보았을 때 CCTV 시야각에서 사람이 지나갈 때 적어도 한번 이상은 디텍션하는 것을 확인할 수 있었다.  
<br>
<br>

## Faster R-CNN의 최고성능
early stop을 적용하지 않은 방식으로 20000iteration을 돌린 결과를 이어받아 early stop을 적용하여 3950 iteration까지 돌아간 뒤 멈춘 결과이다. best loss는 0.16800019922050932 이었다. early stop을 적용하지 않고 50000 iteration을 돌린 결과보다 성능이 좋다.  

```
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 59.357 | 91.427 | 68.213 | 51.822 | 45.529 | 63.570 |
```

```
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| with       | 59.005 | No         | 59.709 |
```
<br>
<br>

## Using Model
만들어진 모델을 정성평가 추론해본다.  

`images` 경로 내의 모든 이미지 파일들을 읽어서 추론하고 평균 추론 시간을 반환한다.  

- `metadata` : 등록된 훈련 데이터셋
- `threshold` : 카테고리를 구분하기위한 기준. 0과 1사이의 값

```python
# Using Model for Inference
## Load Model Weights from trained weights
## Predictor Setup
## Predict and Visualize the result
import glob
import cv2
import matplotlib.pyplot as plt
import time
times = []

def using_model(metadata, threshold):
	"""
  inference model and check average inference time
  :param metadata: Registered train dataset
  :param threshold: Criteria between 0 and 1 to judge the category
  :return:
  """
  cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, 'model_final.pth')
  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = threshold   
  cfg.DATASETS.TEST = (metadata.name, )
  predictor = DefaultPredictor(cfg)

  images = [cv2.imread(file) for file in glob.glob("/gdrive/MyDrive/img/*.png")]

  dataset_dicts = DatasetCatalog.get(metadata.name)
  for i in range(len(images)):
    start_time = time.time()
    im = images[i]
    outputs = predictor(im)
    delta = time.time() - start_time
    times.append(delta)
    v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=0.8)
    v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    plt.figure(figsize = (14, 10))
    plt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))
    plt.show()
  mean_delta = np.array(times).mean()
  fps = 1 / mean_delta
  print("평균 추론 sec:{:.2f},fps:{:.2f}".format(mean_delta, fps)) # 평균 추론 속도
  print(outputs['instances'].pred_classes)
  print(outputs['instances'].pred_boxes)
```
<br>
<br>

## Torchserve로 필요한 파일 넘기기

학습 결과를 torchserve에서 사용하기 위해서는 pth파일과 cfg.dump의 결과를 텍스트 파일로 만들어 넘겨주면 된다.  

```python
cfg.dump()
f = open("cfgdump.txt", 'w')
```
<br>
<br>

## ⚠️ Confusion Matrix

confusion matrix를 통해 false negative, false positive의 값을 알아보고 AP가 더 개선되지 않는 이유를 찾아보고자 했다.  

Detectron2로 학습한 결과에서 이를 계산하기 위해서는 따로 구현이 필요한데, ground truth가 되는 dataset_dicts와 prediction이 되는 metrics.json의 key-value형식을 맞춰주지 않고 그동안 모델을 학습해왔던 것이 문제가 됐다.  

결국 완전한 confusion matrix의 값을 도출하진 못했고, Detectron2에서 제공하는(\detectron2-master\detectron2-master\detectron2\modeling\roi_heads\fast_rcnn)의 `num_false_negative` 값을 metrics.json에 저장하여 false negative를 확인할 수 있었으나, 가장 중요한 false positive를 확인할 수 없어 아쉽다.  