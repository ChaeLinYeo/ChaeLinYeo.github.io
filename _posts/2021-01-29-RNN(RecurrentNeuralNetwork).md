---
title : "[Deep Learning: 신경망의 기초]RNN(Recurrent Neural Network)"
data : 2021-01-29 00:15:28 -0400
categories : 프로그래머스인공지능스쿨
use_math: true
---
# 순환 신경망(RNN)
## 순차 데이터
- 많은 응용
    - 심전도 신호를 분석하여 심장 이상 유무 판정
    - 주식 시세 분석하여 사고 파는 시점 결정
    - 음성 인식을 통한 지능적인 인터페이스 구축
    - 기계 번역기 또는 자동 응답 장치 제작
    - 유전자 열 분석을 통한 치료 계획 수립 등
<br>

## 순차 데이터의 표현
- 순차 데이터의 일반적 표기
    - 벡터의 요소가 벡터
    - 온라인 숫자의 요소는 1차원, 심전도의 요소는 3차원
        - 예를 들어 심전도 신호는 초당 100번 샘플링하고 2분간 측정한다면 길이는 T=12000, x = ((0.3, 0.1, 0.2), ...)^T
    - 위의 예시의 x가 여러개 모인 것이 훈련집합 X가 된다.
- 대표적인 순차 데이터인 문자열의 표현
    - 기계번역의 경우
        - 입력 x가 "April is the cruelest month."이고
        - 출력 y가 "사월은 가장 잔인한 달"일 때, 어떻게 표현할까?
    - 사전(dictionary or term)을 사용하여 표현
        - 사전 구축 방법
            - 사람이 사용하는 단어를 모아 구축
            - 또는 주어진 말뭉치를 분석하여 단어를 자동 추출하여 구축
        - 사전을 사용한 텍스트 순차 데이터의 표현 방법
            - 단어가방(BoW, Bag of Words)
            - 원핫 코드(one-hot code): 특정 기준에만 1, 나머지는 0
            - 단어 임베딩(word embedding): 워드 임베딩을 가장 많이 씀
- 단어 가방
    - 단어 각각의 빈도수를 세어 m차원의 벡터로 표현(m은 사전 크기)
    - 한계
        - 정보 검색에 주로 사용되지만, 기계 학습에서는 부적절
        - "April is the cruelest month"와 "The cruelest month is April"은 같은 특징 벡터로 표현되어 시간성 정보가 사라짐
- 원핫 코드
    - 해당 단어의 위치만 1로 표시(m차원 벡터 요소들 중)
    - 단어 가방에서 해결하지 못한 시간성 정보가 보존됨.
    - 한계
        - 한 단어를 표현하는 데 m차원 벡터를 사용하는 비효율. 고차원이 됨
        - 단어 간의 유사성을 측정할 수 없음
- 단어 임베딩
    - 단어 사이의 상호작용을 분석하여 새로운 공간으로 변환(보통 m  보다 훨씬 낮은 차원으로 변환)
    - 변환 과정은 학습이 말뭉치를 훈련집합으로 사용하여 알아냄
    - word2vec 등
<br>

## 순차 데이터의 특성
- 특징이 나타나는 순서가 중요
    - "아버지가 방에 들어가신다" != "아버지 가방에 들어가신다"
    - 비순차 데이터에서는 순서를 바꾸어도 무방
- 샘플마다 길이가 다름
    - 순환 신경망은 은닉층에 순환 연결을 부여하여 가변 길이 수용
- **문맥 의존성**
    - 비순차 데이터는 공분산이 특징 사이의 의존성을 나타냄
    - 순차 데이터에서는 공분산은 의미가 없고, 대신 문맥 의존성이 중요함
        - "그녀는 점심때가 다 되어서야... 점심을 먹었는데, 철수는..."에서 "그녀는"과 "먹었는데"는 강한 문맥 의존성을 가짐
        - 특히 이 경우 둘 사이의 간격이 크므로 장기 의존성이라 부름 <- LSTM으로 처리(RNN 모델 중 하나)
<br>

## 순환 신경망(RNN, Recurrent neural network)
- 순환 신경망(RNN)이 갖추어야 할 세 가지 필수 기능
    - 시간성: 특징을 순서대로 한 번에 하나씩 입력해야 한다
    - 가변 길이: 길이가 T인 샘플을 처리하려면 은닉층이 T번 나타나야 한다. T는 가변적이다.
    - 문맥 의존성: 이전 특징 내용을 기억하고 있다가 적절한 순간에 활용해야 한다.
<br>

## RNN의 구조
- 기존 깊은 신경망과 유사
    - 입력층, 은닉층, 출력층을 가짐
- 다른 점은 은닉층이 순환 연결을 가진다는 점
    - 시간성, 가변 길이, 문맥 의존성을 모두 처리할 수 있음
    - 순환 연결은 t-1 순간에 발생한 정보를 t 순간으로 전달하는 역할 
- 수식으로 쓰면, $h^{(t)} = f(h^{(t-1)}, x^{(t)}; \Theta)$
    - t=1 순간에 계산하고, 그 결과를 가지고 t=2 순간에 계산하고, 그 결과를 가지고 t=3 순간에 계산하고, ..., T순간까지 반복
    - 일반적으로 t순간에는 t-1 순간의 은닉층 값(상태) $h^{(t-1)}$과 t 순간의 입력 $x^{(t)}$를 받아 $h^{(t)}$로 전환함
    - $\Theta$는 순환 신경망의 매개변수
